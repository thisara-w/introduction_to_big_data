{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Not Only SQL Databases\n",
        "NoSQL databases offer an alternative to relational databases by storing and querying data outside traditional structures, such as tabular formats, often using JSON documents. This schema-less design enables rapid scalability for managing large, unstructured datasets. Additionally, NoSQL databases are distributed, ensuring data availability and reliability across multiple servers, supporting modern web applications' demands for speed and scalability in cloud, big data, and mobile environments. The choice between relational and non-relational databases depends on specific use cases.\n",
        "\n",
        "Here we discuss 3 types of NoSQL databases.\n",
        "\n",
        "1. Document databases\n",
        "2. Wide-column databases\n",
        "3. Columnar databases\n",
        "4. Graph databases\n"
      ],
      "metadata": {
        "id": "0QBuhPEw8Qyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip install \"requests\" \"pyarrow\" \"neo4j\""
      ],
      "metadata": {
        "id": "QCErL2PSGtMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Document Databases\n",
        "\n",
        "Document databases store data as documents, typically in JSON, XML, or BSON formats, making them ideal for managing semi-structured data. This structure keeps data together, reducing translation needs for application usage, and offers flexibility as schemas don't need to match across documents. However, complex transactions may lead to data corruption.\n",
        "\n",
        "Popular for content management systems and user profiles, MongoDB is a notable example. Here we look at how we can connect and query documents from a MongoDB database using a data API.\n",
        "\n"
      ],
      "metadata": {
        "id": "QrUR72ur-_Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "url = 'https://ap-southeast-1.aws.data.mongodb-api.com/app/data-gldcs/endpoint/data/v1/action/find'\n",
        "api_key = 'sdP21rq0OCSiHILoR0rELTxFi7aCLB6ObIJHACEUE452kqoOXYcESPacFThmsOOV'"
      ],
      "metadata": {
        "id": "SmjL4hx1Q6tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create the API body\n",
        "\n",
        "The body of the API request includes details avout the database, collection, filter clauses and field projections."
      ],
      "metadata": {
        "id": "BIpjI34NA7pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"dataSource\": \"Cluster0\",\n",
        "    \"database\": \"sample_airbnb\",\n",
        "    \"collection\": \"listingsAndReviews\",\n",
        "    \"limit\": 5555,\n",
        "    \"filter\": {\n",
        "        \"price\": {\"$gt\": 500},\n",
        "        \"number_of_reviews\": {\"$lt\": 10}\n",
        "    },\n",
        "    \"projection\": {\n",
        "        \"name\": True,\n",
        "        \"reviews\": True\n",
        "    }\n",
        "}\n",
        "payload = json.dumps(data)"
      ],
      "metadata": {
        "id": "AjwRbrA7HCkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload"
      ],
      "metadata": {
        "id": "e4oxfv7OaYSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'api-key': api_key,\n",
        "    'Content-Type': 'application/json',\n",
        "    'Access-Control-Request-Headers': '*'\n",
        "}"
      ],
      "metadata": {
        "id": "i3v17h1tSJb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 API Response\n",
        "\n",
        "Status 200 means OK or a successful API reposnse."
      ],
      "metadata": {
        "id": "ZgxQQS6-BYTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(url, headers=headers, data=payload)\n",
        "response.raise_for_status()\n",
        "print(response)"
      ],
      "metadata": {
        "id": "4xrJROYdSMNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 JSON Output\n",
        "\n",
        "One of the most common data format of the APIs is JSON format. In python we can dump the JSON response to a text file and save it in the disk."
      ],
      "metadata": {
        "id": "THEOL9spBhHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = response.json()\n",
        "with open('results.json','w') as f:\n",
        "  json.dump(result,f)"
      ],
      "metadata": {
        "id": "5_uik7E2Sv1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(result, indent=4, sort_keys=True))\n",
        "documents = result[\"documents\"]\n",
        "print(\"There are\",len(documents), \"properties\")"
      ],
      "metadata": {
        "id": "ll51QGWFdmKI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Columnar Databases\n",
        "Columnar databases store data in columns, allowing users to access specific columns without allocating memory for irrelevant data. They address limitations of key-value and document stores, but their complexity makes them less suitable for new teams and projects. Examples include Apache HBase, built on Hadoop Distributed File System for storing sparse data sets in big data applications, and Apache Cassandra, designed for managing large data across multiple servers and data centers, used in social networking and real-time analytics."
      ],
      "metadata": {
        "id": "0-nVo37NB0bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Apache Cassandra\n"
      ],
      "metadata": {
        "id": "Loua3lll7S1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cassandra is a NoSQL distributed database. By design, NoSQL databases are lightweight, open-source, non-relational, and largely distributed. Counted among their strengths are horizontal scalability, distributed architectures, and a flexible approach to schema definition.\n",
        "\n",
        "NoSQL databases enable rapid, ad-hoc organization and analysis of extremely high-volume, disparate data types. Thatâ€™s become more important in recent years, with the advent of Big Data and the need to rapidly scale databases in the cloud. Cassandra is among the NoSQL databases that have addressed the constraints of previous data management technologies, such as SQL databases.https://cassandra.apache.org/_/cassandra-basics.html"
      ],
      "metadata": {
        "id": "6Tj95O2WKgiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade astrapy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0F9uKxz0-wtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astrapy import DataAPIClient\n",
        "\n",
        "# Initialize the client\n",
        "client = DataAPIClient(\"AstraCS:BsxEcxZwtiLMEqiOAQChJldz:6df6b63168171b9ce1223596948ee1e59d04f609e1edf4b52ea0d4a140e42c93\")\n",
        "db = client.get_database_by_api_endpoint(\n",
        "  \"https://7554c06f-dcf1-412d-81a2-c5e7075f5497-us-east-2.apps.astra.datastax.com\"\n",
        ")\n",
        "\n",
        "print(f\"Connected to Astra DB: {db.list_collection_names()}\")"
      ],
      "metadata": {
        "id": "BPLOuqut_JY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = db.get_collection(\"movie_reviews\")\n",
        "print(collection.find_one())"
      ],
      "metadata": {
        "id": "FFb5rjOENPVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = collection.find({\"criticname\": \"Todd Jorgenson\"})\n",
        "for review in reviews:\n",
        "    print(review['title'])\n"
      ],
      "metadata": {
        "id": "t4vq-zERNBUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotten_reviews = collection.find({\n",
        "    \"reviewstate\": \"rotten\"\n",
        "})\n",
        "\n",
        "for review in rotten_reviews:\n",
        "    print(review)\n"
      ],
      "metadata": {
        "id": "7ozu9tYPOtV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Apache Arrow\n",
        "Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead. https://arrow.apache.org/overview/.\n",
        "\n",
        "Apache arrow is in-memory storage and lazily loads data when iterated to it, making latency very small, and its table format storage also allows me to use simple filters. Plus it can interact with GPU memory as well for data processing. And it doesn't have any serialisation/de-serialisation overheads, as compared to redis which sores data in-memory as bytes and for my usecase it required serialisation/de-serialisation overheads."
      ],
      "metadata": {
        "id": "epVyAfz0ENGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.json"
      ],
      "metadata": {
        "id": "1efxIxe0WGnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size_10MB = 10<<20\n",
        "read_options = pyarrow.json.ReadOptions(\n",
        "    block_size = block_size_10MB\n",
        ")\n",
        "reviews =  pyarrow.json.read_json('/content/results.json', read_options=read_options)\n",
        "print(reviews)"
      ],
      "metadata": {
        "id": "b9v3qAxDWspS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Parquet Files\n",
        "Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. Apache Parquet is designed to be a common interchange format for both batch and interactive workloads. It is similar to other columnar-storage file formats available in Hadoop, namely RCFile and ORC."
      ],
      "metadata": {
        "id": "FO0g5gjrE9u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "pq.write_table(reviews,'results.parquet')"
      ],
      "metadata": {
        "id": "K_m8fBZFjR6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pq.read_table('results.parquet')\n",
        "print(reviews)"
      ],
      "metadata": {
        "id": "Prd89YkFmJPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Graph Databases\n",
        "Graph databases store data as nodes, edges, and properties, forming a knowledge graph where any object, place, or person can be a node, and edges define relationships between nodes. For example, a node could represent a client like IBM, and an edge could indicate the customer relationship between IBM and Ogilvy agency. Graph databases are used for managing connections within the graph network. Neo4j is a prominent graph-based database service, offering both open-source and licensed versions with additional features like online backup and high availability extensions."
      ],
      "metadata": {
        "id": "rRYU5JnvFNAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neo4j"
      ],
      "metadata": {
        "id": "MKE1qJqYNAti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "URI = \"neo4j+s://032d4418.databases.neo4j.io\"\n",
        "AUTH = (\"neo4j\", \"en0Kwb2l6tfypPGj4U6ASlvNSEtsuSxT_13ZiyCwNUk\")\n",
        "\n",
        "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
        "    driver.verify_connectivity()"
      ],
      "metadata": {
        "id": "C51uyFBbsZ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records, summary, keys = driver.execute_query(\n",
        "    \"MATCH (m:movies) WHERE m.imdbRating > 8 RETURN m.title as title\",\n",
        "    database_=\"neo4j\"\n",
        ")\n",
        "\n",
        "for title in records:\n",
        "    print(title)\n",
        "\n",
        "print(\"The query `{query}` returned {records_count} records in {time} ms.\".format(\n",
        "    query=summary.query, records_count=len(records),\n",
        "    time=summary.result_available_after,\n",
        "))"
      ],
      "metadata": {
        "id": "Kjp2j_LhuPsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "DataFrame(records)\n"
      ],
      "metadata": {
        "id": "p6XGK5jC4zhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records, summary, keys = driver.execute_query(\n",
        "    \"MATCH (p:person)-->(m:movies) WHERE m.imdbRating > 8 RETURN p.name\",\n",
        "    database_=\"neo4j\"\n",
        ")\n",
        "\n",
        "for name in records:\n",
        "    print(name)\n",
        "\n",
        "print(\"The query `{query}` returned {records_count} records in {time} ms.\".format(\n",
        "    query=summary.query, records_count=len(records),\n",
        "    time=summary.result_available_after,\n",
        "))\n",
        "DataFrame(records)"
      ],
      "metadata": {
        "id": "oz7RImeE63GB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}